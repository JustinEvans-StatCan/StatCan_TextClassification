{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process as gp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code referenced from github.com/thuijskens/bayesian-optimization\n",
    "\n",
    "# method sample_loss() is what is used to train and evaluate fasttext model\n",
    "# last cell runs the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuning_params(bounds, hyper_params):\n",
    "    discrete = [\"minn\",\"maxn\",\"wordNgrams\",\"epoch\",\"minCount\",\"minCountLabel\",]\n",
    "    params = []\n",
    "    for index, item in enumerate(bounds):\n",
    "        if hyper_params[index] in discrete:\n",
    "            if hyper_params[index] == \"minn\" and \"maxn\" in hyper_params and hyper_params.index(\"minn\") > hyper_params.index(\"maxn\"):\n",
    "                params.append(np.random.randint(low = item[0],high = min(bounds[hyper_params.index(\"maxn\")][0],item[1])+1))\n",
    "            elif hyper_params[index] == \"maxn\" and \"minn\" in hyper_params and hyper_params.index(\"maxn\") > hyper_params.index(\"minn\"):\n",
    "                params.append(np.random.randint(low = max(item[0],bounds[hyper_params.index(\"minn\")][1] ),high =item[1]+1))\n",
    "            else:\n",
    "                params.append(np.random.randint(low = item[0],high = item[1]+1))\n",
    "        else:\n",
    "            params.append(np.random.uniform(low = item[0],high = item[1]))\n",
    "\n",
    "    return np.array(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss,bounds, hyper_params,\n",
    "                                   greater_is_better=False, n_restarts=25):\n",
    "\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for i in range(n_restarts):\n",
    "        starting_point = get_tuning_params(bounds=bounds,hyper_params=hyper_params)\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimisation(n_iters, sample_loss, tuning_vars , n_pre_samples=5, alpha=1e-5, epsilon=1e-7, saved_df=None):\n",
    "   \n",
    "    #tuning vars list of lists\n",
    "    #bounds, np array (matrix)\n",
    "    #hyper_params list of params\n",
    "    temp_bounds_list = []\n",
    "    hyper_params = []\n",
    "    for index, item in enumerate(tuning_vars):\n",
    "        hyper_params.append(item[0])\n",
    "        temp_bounds_list.append(np.array([item[1],item[2]]))\n",
    "    \n",
    "    bounds =np.array(temp_bounds_list)\n",
    "\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "\n",
    "    if saved_df is None:\n",
    "        for i in range (n_pre_samples):\n",
    "            params = get_tuning_params(bounds=bounds,hyper_params=hyper_params)\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params,hyper_params))\n",
    "    else:\n",
    "        numpy_matr = saved_df.values\n",
    "\n",
    "        for i in range(numpy_matr.shape[0]):\n",
    "            saved_row = numpy_matr[i][:-1].copy()\n",
    "            saved_y =  numpy_matr[i][-1].copy()\n",
    "            x_list.append(saved_row)\n",
    "            y_list.append(float(saved_y))\n",
    "\n",
    "    \n",
    "    \n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "    matr = np.column_stack((xp,yp))\n",
    "    col_names = hyper_params.copy()\n",
    "    col_names.append(\"y\")\n",
    "    df = pd.DataFrame(matr,columns=col_names)\n",
    "    df.to_csv(\"test.csv\")\n",
    "\n",
    "    \n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        next_sample = sample_next_hyperparameter(acquisition_func=expected_improvement, gaussian_process=model,\n",
    "                                                 evaluated_loss=yp, greater_is_better=True, bounds=bounds,\n",
    "                                                 hyper_params=hyper_params, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = get_tuning_params(bounds=bounds,hyper_params=hyper_params)\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample,hyper_params,n+1)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "        \n",
    "        matr = np.column_stack((xp,yp)) #TODO fix bad practice here\n",
    "        col_names = hyper_params.copy()\n",
    "        col_names.append(\"y\")\n",
    "        df = pd.DataFrame(matr,columns=col_names)\n",
    "        df.to_csv(\"test.csv\")\n",
    "\n",
    "    return xp, yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import os\n",
    "\n",
    "def sample_loss(points, hyperparams,n=0):\n",
    "    \n",
    "#     print(type(points),type(hyperparams))\n",
    "#     print(points,hyperparams)\n",
    "    #default values - some hyperparamters missing\n",
    "    minCount =1\n",
    "    minCountLabel = 0\n",
    "    wordNgrams = 1\n",
    "    minn = 0\n",
    "    maxn = 0\n",
    "    lr = 0.1\n",
    "    dim = 300\n",
    "    epoch = 5\n",
    "    \n",
    "    train_data = os.path.join(os.getenv(\"DATADIR\",\"\"),\"train_noc_1_level.txt\")\n",
    "    test_data = os.path.join(os.getenv(\"DATADIR\",\"\"),\"test_noc_1_level.txt\")\n",
    "    \n",
    "    for i in range(len(hyperparams)):\n",
    "        key = hyperparams[i]\n",
    "        value = points[i]\n",
    "        if key == \"minCount\":\n",
    "            minCount = int(round(value))\n",
    "        elif key == \"minCountLabel\":\n",
    "            minCountLabel = int(round(value))\n",
    "        elif key == \"wordNgrams\":\n",
    "            wordNgrams = int(round(value))\n",
    "        elif key == \"minn\":\n",
    "            minn= int(round(value))\n",
    "        elif key == \"maxn\":\n",
    "            maxn = int(round(value))\n",
    "        elif key ==\"lr\":\n",
    "            lr = round(value,2)\n",
    "        elif key == \"dim\":\n",
    "             dim = int(round(value))\n",
    "        elif key == \"epoch\":\n",
    "             epoch = int(round(value))\n",
    "        else:\n",
    "             raise Exception(\"Invalid Hyperparameter: \",key)\n",
    "\n",
    "    \n",
    "        \n",
    "    model = fastText.train_supervised(input=train_data,minCount = minCount, minCountLabel= minCountLabel, wordNgrams=wordNgrams, minn = minn, maxn=maxn,lr=lr,dim=dim,epoch=epoch)\n",
    "\n",
    "    print(\"Iteration\",n,\"complete\")\n",
    "    return model.test(test_data,1)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tune_variables = [[\"minn\",1,4],[\"maxn\",1,6], [\"wordNgrams\",1,5], [\"dim\",100,100],[\"epoch\",10,100],[\"lr\",0,1],[\"minCount\",0,8] ]\n",
    "\n",
    "xp,yp = bayesian_optimisation(n_iters=20,sample_loss=sample_loss,tuning_vars=tune_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xp)\n",
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
